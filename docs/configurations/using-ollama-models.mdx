---
title: "Using Ollama Models"
description: "Follow these steps to generate presentations using Ollama"
---

## ðŸ”Œ Run Presenton with an Ollama Model (Fully Offline)

Presenton supports fully offline operation using open-source models via [Ollama](https://ollama.com/). This allows you to generate presentations without relying on cloud APIs â€” keeping your data private and costs low.

### ðŸš€ Example: Run Presenton with Ollama

```bash
docker run -it --name presenton -p 5000:80 \
  -e LLM="ollama" \
  -e OLLAMA_MODEL="llama3.2:3b" \
  -e PEXELS_API_KEY="your_pexels_api_key" \
  -e CAN_CHANGE_KEYS="false" \
  -v "./user_data:/app/user_data" \
  ghcr.io/presenton/presenton:latest
```

### ðŸš€ Example: Run Presenton with you own Ollama server

```bash
docker run -it --name presenton -p 5000:80 \
  -e LLM="ollama" \
  -e OLLAMA_MODEL="llama3.2:3b" \
  -e OLLAMA_URL="http://XXXXXXXXXXXXX" \
  -e PEXELS_API_KEY="your_pexels_api_key" \
  -e CAN_CHANGE_KEYS="false" \
  -v "./user_data:/app/user_data" \
  ghcr.io/presenton/presenton:latest
```


### ðŸ§¾ Ollama Environment Variables

* **`LLM="ollama"`**
  Select Ollama as the LLM backend.

* **`OLLAMA_MODEL`**
  Required. The Ollama model to use (e.g., `llama3.2:3b`, `mistral`, `phi3`, etc.).
  *Example:*

  ```bash
  OLLAMA_MODEL="llama3.2:3b"
  ```

* **`OLLAMA_URL`**
  Optional. Set this if you're running Ollama outside Docker or on a custom host.
  *Example:*

  ```bash
  OLLAMA_URL="http://XXXXXXXXXXXX"
  ```

* **`PEXELS_API_KEY`**
  Optional but recommended. Used to fetch stock images for enhanced visuals.
  *Example:*

  ```bash
  PEXELS_API_KEY="vzXXXXXXXXXXXXXX"
  ```

> ðŸ’¡ **Note:** Provide a valid **Pexels API key** for image generation when using Ollama models.
> You can get a free API key at https://www.pexels.com/api/

> âœ… Add `--gpus=all` to enable GPU acceleration (see [Using GPU](/docs/configurations/using-gpu)).

### ðŸ§  Supported Ollama Models

| Model               | Size   | Graph Support |
| ------------------- | ------ | ------------- |
| **Llama Models**    |        |               |
| `llama3:8b`         | 4.7 GB | âŒ No          |
| `llama3:70b`        | 40 GB  | âœ… Yes         |
| `llama3.1:8b`       | 4.9 GB | âŒ No          |
| `llama3.1:70b`      | 43 GB  | âœ… Yes         |
| `llama3.1:405b`     | 243 GB | âœ… Yes         |
| `llama3.2:1b`       | 1.3 GB | âŒ No          |
| `llama3.2:3b`       | 2 GB   | âŒ No          |
| `llama3.3:70b`      | 43 GB  | âœ… Yes         |
| `llama4:16x17b`     | 67 GB  | âœ… Yes         |
| `llama4:128x17b`    | 245 GB | âœ… Yes         |
| **Gemma Models**    |        |               |
| `gemma3:1b`         | 815 MB | âŒ No          |
| `gemma3:4b`         | 3.3 GB | âŒ No          |
| `gemma3:12b`        | 8.1 GB | âŒ No          |
| `gemma3:27b`        | 17 GB  | âœ… Yes         |
| **DeepSeek Models** |        |               |
| `deepseek-r1:1.5b`  | 1.1 GB | âŒ No          |
| `deepseek-r1:7b`    | 4.7 GB | âŒ No          |
| `deepseek-r1:8b`    | 5.2 GB | âŒ No          |
| `deepseek-r1:14b`   | 9 GB   | âŒ No          |
| `deepseek-r1:32b`   | 20 GB  | âœ… Yes         |
| `deepseek-r1:70b`   | 43 GB  | âœ… Yes         |
| `deepseek-r1:671b`  | 404 GB | âœ… Yes         |
| **Qwen Models**     |        |               |
| `qwen3:0.6b`        | 523 MB | âŒ No          |
| `qwen3:1.7b`        | 1.4 GB | âŒ No          |
| `qwen3:4b`          | 2.6 GB | âŒ No          |
| `qwen3:8b`          | 5.2 GB | âŒ No          |
| `qwen3:14b`         | 9.3 GB | âŒ No          |
| `qwen3:30b`         | 19 GB  | âœ… Yes         |
| `qwen3:32b`         | 20 GB  | âœ… Yes         |
| `qwen3:235b`        | 142 GB | âœ… Yes         |

> âœ… **Graph Support** means the model can generate charts and diagrams in presentations.

### ðŸ“Œ Additional Notes

- Use the `OLLAMA_MODEL` environment variable to select any supported model.
- Ensure your system has enough RAM or GPU memory to handle the model.
- Always include a `PEXELS_API_KEY` for full image generation functionality.